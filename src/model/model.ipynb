{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1bdb7b",
   "metadata": {},
   "source": [
    "# RELANCEZ BIEN LE NOTEBOOK ENTIER A CHAQUE TRAINING POUR GARDER UNE TRACE DES RESULTATS DE CHAQUE MODEL\n",
    "\n",
    "le model.summary est saved dans saved_models/nom_du_run_wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a9dcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../data_cleaning')\n",
    "\n",
    "from time_series_forecasting import create_dataset\n",
    "from dataset import build_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c633324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83cb8292-cdf7-4f02-bacd-8587c2e363b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras.layers import Input,Conv1D,MaxPooling1D,Bidirectional,Dropout,CuDNNLSTM,Dense,Input,LSTM, LeakyReLU\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2aea6",
   "metadata": {},
   "source": [
    "### create wandb_api_key.txt and paste your api key inside &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; from https://wandb.ai/settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54c1e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mduramann\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\theo-/.netrc\n"
     ]
    }
   ],
   "source": [
    "file = open('wandb_api_key.txt', 'r')\n",
    "wandb.login(key=file.readline()) # mettez\n",
    "file.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3090f8c5-2b4d-41e7-b382-6d0ac7e021b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Logiciel_Ã©cole\\GitHub\\AIBSIF\\src\\model\\wandb\\run-20220324_204416-g0chwxsb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/efrai/AI_Based_Sensor_Information_Fusion/runs/g0chwxsb\" target=\"_blank\">proud-wildflower-51</a></strong> to <a href=\"https://wandb.ai/efrai/AI_Based_Sensor_Information_Fusion\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/efrai/AI_Based_Sensor_Information_Fusion/runs/g0chwxsb?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x29a673bf3a0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "learning_rate = 1e-5\n",
    "##To edit: \n",
    "config = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"window_size\": 30,\n",
    "    \"stride\":5,\n",
    "    \"number_imu\":3,\n",
    "    \"bidirectional\":True,\n",
    "    \"lstm_number\": 2,\n",
    "    \"lstm_neurons\":128,\n",
    "    \"dropout\":0.25,\n",
    "    \"dense_number\":1,\n",
    "    \"dense_first_neuron\":64,#puissance de 2\n",
    "    \"activation_dense\":'relu',\n",
    "    \"additional\":\"usage of convolution\"\n",
    "}\n",
    "\n",
    "wandb.init(project=\"AI_Based_Sensor_Information_Fusion\", entity=\"efrai\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf2db605-9562-45f5-bff9-7595a421439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = build_dataset(0, 50, wandb.config['window_size'],wandb.config['stride'],wandb.config['number_imu'])\n",
    "x_test, y_test = build_dataset(51,60, wandb.config['window_size'],wandb.config['stride'],wandb.config['number_imu']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b77774e-c501-4815-b56b-373bff06d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input((wandb.config['window_size'], 6*wandb.config['number_imu'])))\n",
    "model.add(Conv1D(128,11))\n",
    "model.add(Conv1D(128,11))\n",
    "model.add(MaxPooling1D(3))\n",
    "if wandb.config['bidirectional']:\n",
    "    model.add(Bidirectional(CuDNNLSTM(wandb.config['lstm_neurons'], return_sequences=True)))\n",
    "else:\n",
    "    model.add(LSTM(wandb.config['lstm_neurons'], return_sequences=True))\n",
    "for s in range(wandb.config['lstm_number']-1):\n",
    "    if wandb.config['bidirectional']:\n",
    "        model.add(Bidirectional(CuDNNLSTM(wandb.config['lstm_neurons'])))\n",
    "    else:\n",
    "        model.add(LSTM(wandb.config['lstm_neurons']))\n",
    "model.add(Dropout(wandb.config['dropout']))\n",
    "for d in range(wandb.config['dense_number']):\n",
    "    model.add(Dense(int(wandb.config['dense_first_neuron'])/(d+1), activation=wandb.config['activation_dense']))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "    \n",
    "    \n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=opt, loss='mse')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f388958b-dae5-4e81-9e77-144941647427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_2 (Conv1D)           (None, 20, 128)           25472     \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 10, 128)           180352    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 3, 128)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 3, 256)           264192    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 256)              395264    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 866,822\n",
      "Trainable params: 866,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7577f6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "350/350 [==============================] - 15s 18ms/step - loss: 0.1142 - _timestamp: 1648149769.0000 - _runtime: 41.0000 - val_loss: 0.0556- _timestamp: 16\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 5s 15ms/step - loss: 0.0555 - _timestamp: 1648149775.1549 - _runtime: 47.1549 - val_loss: 0.0474\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 5s 15ms/step - loss: 0.0504 - _timestamp: 1648149780.2817 - _runtime: 52.2817 - val_loss: 0.0441\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 5s 15ms/step - loss: 0.0473 - _timestamp: 1648149785.4225 - _runtime: 57.4225 - val_loss: 0.0414\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0439 - _timestamp: 1648149790.8169 - _runtime: 62.8169 - val_loss: 0.0377\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 5s 15ms/step - loss: 0.0401 - _timestamp: 1648149796.2817 - _runtime: 68.2817 - val_loss: 0.0331 - _timestamp: 1648149795.7895 -\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 5s 16ms/step - loss: 0.0351 - _timestamp: 1648149801.5352 - _runtime: 73.5352 - val_loss: 0.0288\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0317 - _timestamp: 1648149807.3099 - _runtime: 79.3099 - val_loss: 0.0266\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0301 - _timestamp: 1648149813.3099 - _runtime: 85.3099 - val_loss: 0.0256\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0292 - _timestamp: 1648149818.9437 - _runtime: 90.9437 - val_loss: 0.0248\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0285 - _timestamp: 1648149824.5070 - _runtime: 96.5070 - val_loss: 0.0247\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 5s 16ms/step - loss: 0.0278 - _timestamp: 1648149830.1831 - _runtime: 102.1831 - val_loss: 0.0241\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0272 - _timestamp: 1648149835.6620 - _runtime: 107.6620 - val_loss: 0.0239\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0268 - _timestamp: 1648149841.3380 - _runtime: 113.3380 - val_loss: 0.0234\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0263 - _timestamp: 1648149847.0563 - _runtime: 119.0563 - val_loss: 0.0230\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0259 - _timestamp: 1648149852.6056 - _runtime: 124.6056 - val_loss: 0.0228\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0255 - _timestamp: 1648149858.3803 - _runtime: 130.3803 - val_loss: 0.0223\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0251 - _timestamp: 1648149864.1831 - _runtime: 136.1831 - val_loss: 0.0221\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0248 - _timestamp: 1648149870.0986 - _runtime: 142.0986 - val_loss: 0.0219\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0245 - _timestamp: 1648149876.2394 - _runtime: 148.2394 - val_loss: 0.0216\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0242 - _timestamp: 1648149882.4225 - _runtime: 154.4225 - val_loss: 0.0214\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0239 - _timestamp: 1648149888.3662 - _runtime: 160.3662 - val_loss: 0.0210\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0236 - _timestamp: 1648149894.2394 - _runtime: 166.2394 - val_loss: 0.0208\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0233 - _timestamp: 1648149900.0423 - _runtime: 172.0423 - val_loss: 0.0206\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0230 - _timestamp: 1648149905.9718 - _runtime: 177.9718 - val_loss: 0.0203\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0227 - _timestamp: 1648149911.9296 - _runtime: 183.9296 - val_loss: 0.0200\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 5s 16ms/step - loss: 0.0225 - _timestamp: 1648149917.6479 - _runtime: 189.6479 - val_loss: 0.0198\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0223 - _timestamp: 1648149923.1690 - _runtime: 195.1690 - val_loss: 0.0196\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0221 - _timestamp: 1648149928.8451 - _runtime: 200.8451 - val_loss: 0.0194\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0218 - _timestamp: 1648149934.8732 - _runtime: 206.8732 - val_loss: 0.0193\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0216 - _timestamp: 1648149941.0986 - _runtime: 213.0986 - val_loss: 0.0190\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0215 - _timestamp: 1648149947.0141 - _runtime: 219.0141 - val_loss: 0.0190\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0213 - _timestamp: 1648149952.6338 - _runtime: 224.6338 - val_loss: 0.0188\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0210 - _timestamp: 1648149958.7324 - _runtime: 230.7324 - val_loss: 0.0186\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0209 - _timestamp: 1648149964.8873 - _runtime: 236.8873 - val_loss: 0.0184\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0207 - _timestamp: 1648149970.6761 - _runtime: 242.6761 - val_loss: 0.0183\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0205 - _timestamp: 1648149976.6901 - _runtime: 248.6901 - val_loss: 0.0183\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0204 - _timestamp: 1648149982.4507 - _runtime: 254.4507 - val_loss: 0.0180\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0202 - _timestamp: 1648149988.3521 - _runtime: 260.3521 - val_loss: 0.0179\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 7s 19ms/step - loss: 0.0201 - _timestamp: 1648149995.1972 - _runtime: 267.1972 - val_loss: 0.0177\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 7s 20ms/step - loss: 0.0197 - _timestamp: 1648150001.9296 - _runtime: 273.9296 - val_loss: 0.0174\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0196 - _timestamp: 1648150008.6197 - _runtime: 280.6197 - val_loss: 0.0173\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 7s 21ms/step - loss: 0.0195 - _timestamp: 1648150015.3662 - _runtime: 287.3662 - val_loss: 0.0173\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 7s 20ms/step - loss: 0.0193 - _timestamp: 1648150022.6338 - _runtime: 294.6338 - val_loss: 0.0169\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0191 - _timestamp: 1648150029.0986 - _runtime: 301.0986 - val_loss: 0.0167\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0190 - _timestamp: 1648150034.8732 - _runtime: 306.8732 - val_loss: 0.0166\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0188 - _timestamp: 1648150040.5915 - _runtime: 312.5915 - val_loss: 0.0165\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0186 - _timestamp: 1648150046.5211 - _runtime: 318.5211 - val_loss: 0.0164\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0184 - _timestamp: 1648150052.5775 - _runtime: 324.5775 - val_loss: 0.0161\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0183 - _timestamp: 1648150058.3944 - _runtime: 330.3944 - val_loss: 0.0160\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 5s 16ms/step - loss: 0.0180 - _timestamp: 1648150063.8873 - _runtime: 335.8873 - val_loss: 0.0158\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0180 - _timestamp: 1648150069.4789 - _runtime: 341.4789 - val_loss: 0.0156\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0178 - _timestamp: 1648150075.7465 - _runtime: 347.7465 - val_loss: 0.0154\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0176 - _timestamp: 1648150081.4930 - _runtime: 353.4930 - val_loss: 0.0154\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 5s 16ms/step - loss: 0.0175 - _timestamp: 1648150087.1408 - _runtime: 359.1408 - val_loss: 0.0152\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0173 - _timestamp: 1648150092.6056 - _runtime: 364.6056 - val_loss: 0.0150\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0171 - _timestamp: 1648150098.3944 - _runtime: 370.3944 - val_loss: 0.0150\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 7s 19ms/step - loss: 0.0170 - _timestamp: 1648150104.5775 - _runtime: 376.5775 - val_loss: 0.0148\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 6s 19ms/step - loss: 0.0169 - _timestamp: 1648150111.0845 - _runtime: 383.0845 - val_loss: 0.0147\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0168 - _timestamp: 1648150117.3803 - _runtime: 389.3803 - val_loss: 0.0146\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0166 - _timestamp: 1648150123.7746 - _runtime: 395.7746 - val_loss: 0.0145\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0164 - _timestamp: 1648150129.7746 - _runtime: 401.7746 - val_loss: 0.0143\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0164 - _timestamp: 1648150135.7042 - _runtime: 407.7042 - val_loss: 0.0143\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0162 - _timestamp: 1648150141.4225 - _runtime: 413.4225 - val_loss: 0.0144\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0160 - _timestamp: 1648150147.1831 - _runtime: 419.1831 - val_loss: 0.0141\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0160 - _timestamp: 1648150153.0845 - _runtime: 425.0845 - val_loss: 0.0141\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0159 - _timestamp: 1648150158.7183 - _runtime: 430.7183 - val_loss: 0.0139\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0158 - _timestamp: 1648150164.7324 - _runtime: 436.7324 - val_loss: 0.0140\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0157 - _timestamp: 1648150170.8732 - _runtime: 442.8732 - val_loss: 0.0138\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0156 - _timestamp: 1648150176.7887 - _runtime: 448.7887 - val_loss: 0.0136\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0155 - _timestamp: 1648150182.8169 - _runtime: 454.8169 - val_loss: 0.0137\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0154 - _timestamp: 1648150188.7465 - _runtime: 460.7465 - val_loss: 0.0135\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0153 - _timestamp: 1648150195.0141 - _runtime: 467.0141 - val_loss: 0.0135\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0152 - _timestamp: 1648150200.7887 - _runtime: 472.7887 - val_loss: 0.0133\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0151 - _timestamp: 1648150206.8451 - _runtime: 478.8451 - val_loss: 0.0133\n",
      "Epoch 76/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0151 - _timestamp: 1648150213.2113 - _runtime: 485.2113 - val_loss: 0.0132\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0149 - _timestamp: 1648150219.2394 - _runtime: 491.2394 - val_loss: 0.0132\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0149 - _timestamp: 1648150225.0282 - _runtime: 497.0282 - val_loss: 0.0131\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0149 - _timestamp: 1648150230.6620 - _runtime: 502.6620 - val_loss: 0.0130\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0148 - _timestamp: 1648150236.2394 - _runtime: 508.2394 - val_loss: 0.0129\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0146 - _timestamp: 1648150241.9296 - _runtime: 513.9296 - val_loss: 0.0130\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0146 - _timestamp: 1648150247.8310 - _runtime: 519.8310 - val_loss: 0.0128\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0145 - _timestamp: 1648150254.4789 - _runtime: 526.4789 - val_loss: 0.0129\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0145 - _timestamp: 1648150260.4930 - _runtime: 532.4930 - val_loss: 0.0127\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0144 - _timestamp: 1648150266.3099 - _runtime: 538.3099 - val_loss: 0.0127\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0143 - _timestamp: 1648150272.3099 - _runtime: 544.3099 - val_loss: 0.0126\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0142 - _timestamp: 1648150278.8592 - _runtime: 550.8592 - val_loss: 0.0125\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0142 - _timestamp: 1648150284.6479 - _runtime: 556.6479 - val_loss: 0.0124\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0141 - _timestamp: 1648150290.7887 - _runtime: 562.7887 - val_loss: 0.0125\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0140 - _timestamp: 1648150296.6056 - _runtime: 568.6056 - val_loss: 0.0124\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 6s 18ms/step - loss: 0.0140 - _timestamp: 1648150302.8169 - _runtime: 574.8169 - val_loss: 0.0125\n",
      "Epoch 92/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0139 - _timestamp: 1648150308.8873 - _runtime: 580.8873 - val_loss: 0.0125\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0138 - _timestamp: 1648150314.7183 - _runtime: 586.7183 - val_loss: 0.0122\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0138 - _timestamp: 1648150320.5352 - _runtime: 592.5352 - val_loss: 0.0122\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0137 - _timestamp: 1648150326.4225 - _runtime: 598.4225 - val_loss: 0.0123\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 5s 16ms/step - loss: 0.0137 - _timestamp: 1648150332.2113 - _runtime: 604.2113 - val_loss: 0.0120\n",
      "Epoch 97/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0136 - _timestamp: 1648150337.7606 - _runtime: 609.7606 - val_loss: 0.0121\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0136 - _timestamp: 1648150343.4648 - _runtime: 615.4648 - val_loss: 0.0121\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 6s 17ms/step - loss: 0.0135 - _timestamp: 1648150349.0563 - _runtime: 621.0563 - val_loss: 0.0120\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 6s 16ms/step - loss: 0.0135 - _timestamp: 1648150354.9577 - _runtime: 626.9577 - val_loss: 0.0119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29a0cc903a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.1,\n",
    "    batch_size = wandb.config['batch_size'],\n",
    "    epochs=wandb.config['epochs'],\n",
    "    callbacks=[wandb.keras.WandbCallback(log_batch_frequency=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1c3d8d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23128/2032869428.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "wandb.save('model'+str(wandb.run.name)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db36ee04-0461-423e-aed1-b3e8969da41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">proud-wildflower-51</strong>: <a href=\"https://wandb.ai/efrai/AI_Based_Sensor_Information_Fusion/runs/g0chwxsb\" target=\"_blank\">https://wandb.ai/efrai/AI_Based_Sensor_Information_Fusion/runs/g0chwxsb</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220324_204416-g0chwxsb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
